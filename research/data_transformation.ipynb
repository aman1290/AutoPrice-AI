{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "497d0e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9ae23b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\anand\\\\Desktop\\\\reume_projet\\\\AutoPrice-AI'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "cd1d04da",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:/Users/anand/Desktop/reume_projet/AutoPrice-AI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d14692",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d9b308",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "80d03db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataTransformationConfig:\n",
    "    root_dir: str\n",
    "    data_path: str\n",
    "    target_column: str\n",
    "    target_encode_cols: list\n",
    "    test_size: float\n",
    "    random_state: int\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8bd7392b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlProject.constants import *\n",
    "from mlProject.utils.common import read_yaml, create_directories\n",
    "from mlProject import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ff7f185c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath: str = CONFIG_FILE_PATH,\n",
    "        params_filepath: str = PARAMS_FILE_PATH,\n",
    "        schema_filepath: str = SCHEMA_FILE_PATH\n",
    "    ):\n",
    "        try:\n",
    "            self.config = read_yaml(config_filepath)\n",
    "            self.params = read_yaml(params_filepath)\n",
    "            self.schema = read_yaml(schema_filepath)\n",
    "            create_directories([self.config.artifacts_root])\n",
    "            logger.info(\"Configuration loaded successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Config loading failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        try:\n",
    "            # Verify config sections exist\n",
    "            if not hasattr(self.config, 'data_transformation'):\n",
    "                raise ValueError(\"Missing 'data_transformation' in config\")\n",
    "            if not hasattr(self.schema, 'TARGET_COLUMN'):\n",
    "                raise ValueError(\"Missing 'TARGET_COLUMN' in schema\")\n",
    "\n",
    "            config = self.config.data_transformation\n",
    "            schema = self.schema.TARGET_COLUMN\n",
    "\n",
    "            # Set default params if not specified\n",
    "            target_encode_cols = getattr(self.params, 'target_encode_cols', [\"model\"])\n",
    "            test_size = getattr(self.params, 'test_size', 0.25)\n",
    "            random_state = getattr(self.params, 'random_state', 42)\n",
    "\n",
    "            create_directories([config.root_dir])\n",
    "\n",
    "            return DataTransformationConfig(\n",
    "                root_dir=config.root_dir,\n",
    "                data_path=config.data_path,\n",
    "                target_column=schema.name,\n",
    "                target_encode_cols=target_encode_cols,\n",
    "                test_size=test_size,\n",
    "                random_state=random_state\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to create data transformation config: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "93032faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "cd44b243",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformation:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.ownership_map = {\n",
    "            'First Owner': 1,\n",
    "            'Second Owner': 2,\n",
    "            'Third Owner': 3,\n",
    "            'Fourth Owner': 4\n",
    "        }\n",
    "        self.transmission_map = {\n",
    "            'Manual': 0,\n",
    "            'Automatic': 1\n",
    "        }\n",
    "\n",
    "    def _preprocess_data(self, data):\n",
    "        \"\"\"Apply initial preprocessing to data\"\"\"\n",
    "        # Convert mappings\n",
    "        data['ownership'] = data['ownership'].map(self.ownership_map)\n",
    "        data['transmission'] = data['transmission'].map(self.transmission_map)\n",
    "        return data\n",
    "\n",
    "    def train_test_splitting(self):\n",
    "        \"\"\"Complete data transformation pipeline with leakage prevention\"\"\"\n",
    "        try:\n",
    "            # Load data\n",
    "            data = pd.read_csv(self.config.data_path)\n",
    "            logger.info(f\"Original data shape: {data.shape}\")\n",
    "\n",
    "            # Initial split to prevent leakage\n",
    "            train, test = train_test_split(\n",
    "                data,\n",
    "                test_size=self.config.test_size,\n",
    "                random_state=self.config.random_state\n",
    "            )\n",
    "            logger.info(f\"Initial split - Train: {train.shape}, Test: {test.shape}\")\n",
    "\n",
    "            # Apply base preprocessing\n",
    "            train = self._preprocess_data(train)\n",
    "            test = self._preprocess_data(test)\n",
    "\n",
    "            # Target encoding for 'model'\n",
    "            model_encoder = ce.TargetEncoder(cols=['model'], smoothing=5.0)\n",
    "            model_encoder.fit(train['model'], train['price'])\n",
    "            train['model_encoded'] = model_encoder.transform(train['model'])\n",
    "            test['model_encoded'] = model_encoder.transform(test['model'])\n",
    "\n",
    "            # One-hot encoding for other categorical features\n",
    "            ohe_columns = ['make', 'fuel', 'city']\n",
    "            ohe = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
    "            ohe.fit(train[ohe_columns])\n",
    "            \n",
    "            # Transform features\n",
    "            train_ohe = pd.DataFrame(\n",
    "                ohe.transform(train[ohe_columns]),\n",
    "                columns=ohe.get_feature_names_out(ohe_columns),\n",
    "                index=train.index\n",
    "            )\n",
    "            test_ohe = pd.DataFrame(\n",
    "                ohe.transform(test[ohe_columns]),\n",
    "                columns=ohe.get_feature_names_out(ohe_columns),\n",
    "                index=test.index\n",
    "            )\n",
    "            \n",
    "            # Combine encoded data\n",
    "            train = pd.concat([\n",
    "                train.drop(columns=['model'] + ohe_columns),\n",
    "                train_ohe\n",
    "            ], axis=1)\n",
    "            \n",
    "            test = pd.concat([\n",
    "                test.drop(columns=['model'] + ohe_columns),\n",
    "                test_ohe\n",
    "            ], axis=1)\n",
    "\n",
    "            # Save processed data\n",
    "            os.makedirs(self.config.root_dir, exist_ok=True)\n",
    "            train_path = os.path.join(self.config.root_dir, \"train.csv\")\n",
    "            test_path = os.path.join(self.config.root_dir, \"test.csv\")\n",
    "            train.to_csv(train_path, index=False)\n",
    "            test.to_csv(test_path, index=False)\n",
    "\n",
    "            logger.info(f\"Final train shape: {train.shape}, test shape: {test.shape}\")\n",
    "            logger.info(f\"Processed data saved to {self.config.root_dir}\")\n",
    "            return train_path, test_path\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Data transformation failed: {str(e)}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "48e7667e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-16 02:28:50,940: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-06-16 02:28:50,943: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-06-16 02:28:50,946: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2025-06-16 02:28:50,948: INFO: common: created directory at: artifacts]\n",
      "[2025-06-16 02:28:50,949: INFO: 2903215021: Configuration loaded successfully]\n",
      "[2025-06-16 02:28:50,950: INFO: common: created directory at: artifacts/data_transformation]\n",
      "[2025-06-16 02:28:50,963: INFO: 2318304100: Original data shape: (2252, 11)]\n",
      "[2025-06-16 02:28:50,966: INFO: 2318304100: Initial split - Train: (1689, 11), Test: (563, 11)]\n",
      "[2025-06-16 02:28:51,039: INFO: 2318304100: Final train shape: (1689, 36), test shape: (563, 36)]\n",
      "[2025-06-16 02:28:51,040: INFO: 2318304100: Processed data saved to artifacts/data_transformation]\n",
      "[2025-06-16 02:28:51,041: INFO: 2719237422: Pipeline completed successfully]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "        config = ConfigurationManager()\n",
    "        data_transformation_config = config.get_data_transformation_config()\n",
    "        data_transformation = DataTransformation(config=data_transformation_config)\n",
    "        train_path, test_path = data_transformation.train_test_splitting()\n",
    "        logger.info(f\"Pipeline completed successfully\")\n",
    "except Exception as e:\n",
    "        logger.exception(\"Pipeline failed\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3027ce13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-16 02:27:36,508: INFO: 482361250: Starting model training pipeline]\n",
      "[2025-06-16 02:27:36,510: ERROR: 482361250: Data loading failed: [Errno 2] No such file or directory: '/artifacts/data_transformation/train.csv']\n",
      "[2025-06-16 02:27:36,511: ERROR: 482361250: Model training failed]\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\anand\\AppData\\Local\\Temp\\ipykernel_19900\\482361250.py\", line 59, in train\n",
      "    X_train, X_test, y_train, y_test = self._load_data()\n",
      "                                       ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\anand\\AppData\\Local\\Temp\\ipykernel_19900\\482361250.py\", line 22, in _load_data\n",
      "    train_data = pd.read_csv(\"/artifacts/data_transformation/train.csv\")\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\anand\\Desktop\\reume_projet\\AutoPrice-AI\\mlproj\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1026, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\anand\\Desktop\\reume_projet\\AutoPrice-AI\\mlproj\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 620, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\anand\\Desktop\\reume_projet\\AutoPrice-AI\\mlproj\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1620, in __init__\n",
      "    self._engine = self._make_engine(f, self.engine)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\anand\\Desktop\\reume_projet\\AutoPrice-AI\\mlproj\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1880, in _make_engine\n",
      "    self.handles = get_handle(\n",
      "                   ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\anand\\Desktop\\reume_projet\\AutoPrice-AI\\mlproj\\Lib\\site-packages\\pandas\\io\\common.py\", line 873, in get_handle\n",
      "    handle = open(\n",
      "             ^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/artifacts/data_transformation/train.csv'\n",
      "[2025-06-16 02:27:36,513: ERROR: 482361250: Model training pipeline failed]\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\anand\\AppData\\Local\\Temp\\ipykernel_19900\\482361250.py\", line 176, in <module>\n",
      "    results = trainer.train()\n",
      "              ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\anand\\AppData\\Local\\Temp\\ipykernel_19900\\482361250.py\", line 59, in train\n",
      "    X_train, X_test, y_train, y_test = self._load_data()\n",
      "                                       ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\anand\\AppData\\Local\\Temp\\ipykernel_19900\\482361250.py\", line 22, in _load_data\n",
      "    train_data = pd.read_csv(\"/artifacts/data_transformation/train.csv\")\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\anand\\Desktop\\reume_projet\\AutoPrice-AI\\mlproj\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1026, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\anand\\Desktop\\reume_projet\\AutoPrice-AI\\mlproj\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 620, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\anand\\Desktop\\reume_projet\\AutoPrice-AI\\mlproj\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1620, in __init__\n",
      "    self._engine = self._make_engine(f, self.engine)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\anand\\Desktop\\reume_projet\\AutoPrice-AI\\mlproj\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1880, in _make_engine\n",
      "    self.handles = get_handle(\n",
      "                   ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\anand\\Desktop\\reume_projet\\AutoPrice-AI\\mlproj\\Lib\\site-packages\\pandas\\io\\common.py\", line 873, in get_handle\n",
      "    handle = open(\n",
      "             ^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/artifacts/data_transformation/train.csv'\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/artifacts/data_transformation/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[118]\u001b[39m\u001b[32m, line 186\u001b[39m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    185\u001b[39m     logger.exception(\u001b[33m\"\u001b[39m\u001b[33mModel training pipeline failed\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[118]\u001b[39m\u001b[32m, line 176\u001b[39m\n\u001b[32m    174\u001b[39m config = ModelTrainerConfig()\n\u001b[32m    175\u001b[39m trainer = ModelTrainer(config)\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m results = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTraining Results Summary:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    179\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBest Model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults[\u001b[33m'\u001b[39m\u001b[33mbest_model\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[118]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36mModelTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Main training pipeline with model selection\"\"\"\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     58\u001b[39m     \u001b[38;5;66;03m# 1. Load data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     X_train, X_test, y_train, y_test = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m     \u001b[38;5;66;03m# Initialize tracking variables\u001b[39;00m\n\u001b[32m     62\u001b[39m     best_score = \u001b[38;5;28mfloat\u001b[39m(\u001b[33m'\u001b[39m\u001b[33minf\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[118]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mModelTrainer._load_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Load and validate training data\"\"\"\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     train_data = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/artifacts/data_transformation/train.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m     test_data = pd.read_csv(\u001b[33m\"\u001b[39m\u001b[33m/artifacts/data_transformation/test.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m     \u001b[38;5;66;03m# Validate target column exists\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\anand\\Desktop\\reume_projet\\AutoPrice-AI\\mlproj\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\anand\\Desktop\\reume_projet\\AutoPrice-AI\\mlproj\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\anand\\Desktop\\reume_projet\\AutoPrice-AI\\mlproj\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\anand\\Desktop\\reume_projet\\AutoPrice-AI\\mlproj\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\anand\\Desktop\\reume_projet\\AutoPrice-AI\\mlproj\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m    876\u001b[39m             encoding=ioargs.encoding,\n\u001b[32m    877\u001b[39m             errors=errors,\n\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/artifacts/data_transformation/train.csv'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4245ba4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
